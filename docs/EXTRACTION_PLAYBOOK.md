# Extraction Playbook

Step-by-step instructions for AI-driven data extraction and analysis.

**Persona:** `@data` (see CLAUDE.md)

## Overview

Two-stage process with persistent storage of AI-analyzed commits:

1. **Extract** raw git data (ephemeral)
2. **AI analyzes** each commit message and assigns tags, complexity, urgency, and impact
3. **Store** in `processed/` folder (persistent, source of truth)
4. **Aggregate** to dashboard (generated from processed)

## Triggers

| Trigger | Action |
|---------|--------|
| **"hatch the chicken"** | Full reset - delete everything, extract all repos, AI analyzes ALL commits, save to `processed/`, aggregate to dashboard |
| **"feed the chicken"** | Incremental - extract repos, AI analyzes only NEW commits (not in `processed/`), update `processed/`, re-aggregate |

## Commit Processing Strategy

Human-in-the-loop review ensures quality tagging.

### Batch Size
- Process **25 commits per batch** (for review efficiency)
- AI presents analysis, **user reviews and approves**
- After approval: write individual commit files to `processed/<repo>/commits/`, update manifest
- Commits happen on demand (not after every batch)
- If session ends, uncommitted work is lost (commit regularly)

### Review Format

For each batch, AI presents commits like this:

```text
[1/25] abc123
Subject: Fix button placement and performance issues
Body:
- Move "Update Visualization" button below the image
- Fix page freeze on visualization update by removing scrollIntoView
- Optimize DOM manipulation using classList.toggle
- Update help documentation to reflect UI changes

Tags: refactor, bugfix, performance, docs
Complexity: 4 | Urgency: 3 | Impact: user-facing
---
[2/25] def456
...
```

User responds:
- **"approve"** - Save batch, continue to next (no commit yet)
- **"approve and commit"** - Save batch + commit all pending changes
- **"commit"** - Commit all pending changes now
- **"#3 should be feature not refactor"** - AI corrects and re-presents
- **"stop"** - Commit pending changes and end session

### Benefits

- **Accurate**: Human review catches AI mistakes
- **Multi-tag**: AI reads full message, assigns ALL relevant tags
- **Fast**: 25 commits per batch, fewer approval prompts
- **Flexible**: Commit when you want, not after every batch

## File Structure

```text
reports/                      # Raw extracted data (ephemeral, from extract)
  <repo-name>/
    commits.json              # All commits (bulk file)
    commits/                  # Individual commit files (for merge-analysis.js)
      abc1234.json
      def5678.json
      ...
    metadata.json
    data.json

pending/                      # Unprocessed commits (ephemeral, from pending.js)
  <repo-name>/
    batches/                  # Commits NOT in manifest, split into batches
      batch-001.json          # 25 commits per batch
      batch-002.json
      ...

processed/                    # AI-analyzed data (committed to git)
  <repo-name>/
    manifest.json             # Tracks processed SHAs
    commits/                  # Individual commit files (with AI tags)
      abc1234.json
      def5678.json
      ...
    needs-reprocess.json      # Failed analysis attempts (if any)

config/
  repos.json                  # Tracked repositories
  author-map.json             # Author identity mapping

dashboard/
  data.json                   # Aggregated from processed/
```

**Progress tracking via manifest:**
- Each repo has `processed/<repo>/manifest.json` with array of processed SHAs
- Run `node scripts/pending.js` to generate pending batches
- Pending batches contain ONLY commits not in the manifest
- This works regardless of batch boundary shifts from new commits

**What's committed to git:**

- `processed/*/commits/*.json` - AI-analyzed commits (one file per commit)
- `processed/*/manifest.json` - Index of processed SHAs
- `dashboard/data.json` - Aggregated dashboard data

**What's gitignored (working files):**

- `reports/` - Raw extracted data (regenerated by extract)
- `pending/` - Generated pending commits (regenerated)
- `.repo-cache/` - Cloned repositories (only used with `--clone` flag)

---

## Hatch the Chicken (Full Reset)

Use when: Starting fresh, schema changes, or need to reprocess everything.

### Step 1: Clean Slate

```bash
# Delete ALL extracted and processed data
rm -rf processed/
rm -rf reports/

# Delete dashboard aggregated data
rm -f dashboard/commits.json dashboard/data.json dashboard/summary.json dashboard/metadata.json dashboard/contributors.json dashboard/files.json
```

**What gets deleted:**
- `processed/` - All AI-analyzed commits and manifests
- `reports/` - All raw extracted data
- `dashboard/*.json` - Aggregated dashboard data

**What's preserved:**
- `config/repos.json` - List of tracked repos
- `config/author-map.json` - Author identity mapping
- `dashboard/index.html` - Dashboard UI

### Step 2: Extract Each Repository

For each repo in `config/repos.json`:

```bash
# Extract via GitHub API (default, no cloning)
scripts/update-all.sh

# Or clone and extract locally (if you prefer)
scripts/update-all.sh --clone
```

This extracts raw git data to `reports/`.

### Step 3: Generate Pending Batches

Since manifest is empty (full reset), ALL commits will be pending:

```bash
node scripts/pending.js
```

This creates `pending/<repo>/batches/` with all commits split into batches.

### Step 4: AI Analyze Batches (Human Review)

**Same flow as "feed the chicken"** - see Step 3 in that section.

Process `pending/<repo>/batches/batch-NNN.json` files one at a time using `merge-analysis.js`.

**Progress check:**
```bash
node scripts/pending.js      # Shows remaining vs processed
ls processed/<repo>/commits/ # Completed commits
```

**Verification per repo:**
- [ ] Every commit has at least one tag
- [ ] Every commit has complexity 1-5
- [ ] Every commit has urgency 1-5
- [ ] Every commit has impact (internal/user-facing/infrastructure/api)
- [ ] Tags reflect FULL message content (subject + body)

### Step 5: Aggregate to Dashboard

Combine all `processed/` data into dashboard files:

```bash
node scripts/aggregate-processed.js
```

### Step 6: Commit Changes

```bash
git add processed/ dashboard/
git commit -m "chore: hatch the chicken - full extraction"
git push
```

---

## Feed the Chicken (Incremental)

Use when: Adding new commits OR resuming interrupted processing.

### Step 1: Extract Fresh Data

```bash
# Via GitHub API (default, fast, no cloning)
scripts/update-all.sh

# Or via cloning (if needed)
scripts/update-all.sh --clone
```

This extracts ALL commits from each repo into `reports/`.

### Step 2: Generate Pending Batches

```bash
node scripts/pending.js
```

This script:
1. Reads `processed/<repo>/manifest.json` to get already-processed SHAs
2. Compares against freshly extracted commits in `reports/`
3. Generates `pending/<repo>/batches/` with ONLY unprocessed commits

**Output shows:**

```text
social-ad-creator: 66/156 pending (7 batches)
model-pear: 302/302 pending (31 batches)
Total: 368 pending commits in 38 batches
```

### Step 3: AI Analyze Pending Batches (Human Review)

Process pending batch files one at a time:

**Per batch file:**
1. Read from `pending/<repo>/batches/batch-NNN.json`
2. AI proposes tags + complexity + urgency + impact for each commit
3. Present to user for review
4. User approves or corrects
5. On approval, AI outputs **analysis only** and runs merge script:
   ```bash
   cat <<'EOF' | node scripts/merge-analysis.js <repo>
   {"commits": [
     {"sha": "abc123", "tags": ["feature", "ui"], "complexity": 2, "urgency": 1, "impact": "user-facing"},
     {"sha": "def456", "tags": ["bugfix"], "complexity": 1, "urgency": 3, "impact": "user-facing"}
   ]}
   EOF
   ```
6. Script merges analysis with raw git data from `reports/` and saves to `processed/`
7. Commit changes

**User commands:**
- `approve` - Merge analysis via script, continue to next
- `#N tag1, tag2` - Correct tags for commit N, re-present
- `stop` - End session (progress saved via manifest)

**Why merge-analysis.js? (Optimized workflow)**
- AI outputs **only analysis fields** (sha, tags, complexity, urgency, impact)
- Script merges with full git data from `reports/<repo>/commits/<sha>.json`
- **~10x reduction in AI output tokens** (50-80 vs 500-800 per commit)
- Faster processing, lower cost, same quality

**Stop when:**
- All pending batches processed, OR
- User says "stop" (progress saved in manifest)

**When a repo is fully processed (no more pending batches):**
AI automatically runs cleanup for that repo:
```bash
# Clean up redundant files for completed repo
rm -f reports/<repo>/commits.json reports/<repo>/data.json
rm -f reports/<repo>/files.json reports/<repo>/contributors.json reports/<repo>/summary.json
rm -rf pending/<repo>/
```

### Step 4: Re-aggregate to Dashboard

```bash
node scripts/aggregate.js
```

### Step 5: Commit Changes

```bash
git add processed/ dashboard/
git commit -m "chore: feed the chicken - X new commits"
git push
```

### Key Differences from "Hatch"

| Aspect | Hatch the Chicken | Feed the Chicken |
|--------|-------------------|------------------|
| Starting point | Delete everything | Keep existing processed/ |
| Batch source | `pending/` (all commits) | `pending/` (new commits only) |
| Manifest state | Empty (reset) | Contains processed SHAs |
| Handles new commits | N/A (full reset) | Only processes new ones |
| Resume after merge | Start over | Continues seamlessly |

---

## Validation & Reprocessing

### What Gets Validated

`save-commit.js` validates every commit before saving:

**Required metadata fields:**
- `sha` - Commit hash
- `timestamp` - Author date (ISO format)
- `subject` - Commit subject line
- `author` or `author_id` - Author info

**Required analysis fields:**
- `tags` - Array of tags
- `complexity` - Score 1-5
- `urgency` - Score 1-5
- `impact` - One of: internal, user-facing, infrastructure, api

### When Validation Fails

If commits fail validation, `save-commit.js`:
1. Writes failures to `processed/<repo>/needs-reprocess.json`
2. Still saves any valid commits
3. Exits with error code (so it's visible)
4. Prints loud warnings about what failed

**Common causes:**
- AI outputs wrong format (e.g., `complexity: "high"` instead of `complexity: 3`)
- AI omits required fields
- Raw commit not found in `reports/` (extraction issue)

**Both scripts write failures to `needs-reprocess.json`** with:
- SHA and error details
- The analysis that failed (for debugging)
- Attempt count (tracks repeated failures)
- Timestamp of last failure

### Fixing Malformed Commits

If `needs-reprocess.json` files exist, run:

```bash
node scripts/fix-malformed.js
```

This script:
1. Reads `needs-reprocess.json` from each repo
2. Clones/updates repos from `config/repos.json`
3. Extracts git metadata for each malformed commit
4. Merges with existing analysis (tags, complexity, urgency, impact)
5. Saves complete commit objects
6. Re-aggregates dashboard data

**After extraction or feeding:**
AI should check for `needs-reprocess.json` files and run `fix-malformed.js` if any exist.

### Aggregation Validation

`aggregate-processed.js` also validates commits during aggregation:
- Skips malformed commits (missing timestamp, author, etc.)
- Writes skipped commits to `needs-reprocess.json`
- Prints warnings so issues are visible

This catches any commits that slipped through or were corrupted.

---

## Tagging Guidelines

AI reads the full commit message (subject + body) and assigns ALL tags that apply.
Multiple tags per commit is expected and encouraged.

### Tags by Category

**User-Facing Work:**
| Tag | What work was done |
|-----|-------------------|
| `feature` | Built something new for users |
| `enhancement` | Improved existing functionality |
| `bugfix` | Fixed broken behavior |
| `hotfix` | Urgent production fix |
| `ui` | Visual/interface changes |
| `ux` | User experience flow improvements |
| `accessibility` | Accessibility (a11y) improvements |
| `i18n` | Internationalization support |
| `localization` | Translations |

**Code Changes:**
| Tag | What work was done |
|-----|-------------------|
| `refactor` | Restructured code |
| `simplify` | Reduced complexity |
| `removal` | Deleted dead code/files |
| `deprecation` | Marked something as deprecated |
| `migration` | Migrated to new approach/library |
| `naming` | Renamed variables/functions/files |
| `types` | Type definitions/annotations |

**Performance:**
| Tag | What work was done |
|-----|-------------------|
| `performance` | Speed improvements |
| `memory` | Memory optimization |
| `caching` | Added/improved caching |

**Security:**
| Tag | What work was done |
|-----|-------------------|
| `security` | General security work |
| `auth` | Authentication changes |
| `authorization` | Permissions/access control |
| `vulnerability` | Fixed security vulnerability |
| `sanitization` | Input validation |

**Testing:**
| Tag | What work was done |
|-----|-------------------|
| `test-unit` | Unit tests |
| `test-integration` | Integration tests |
| `test-e2e` | End-to-end tests |
| `test-fix` | Fixed broken tests |
| `coverage` | Improved test coverage |
| `mocks` | Test mocks/fixtures |

**Documentation:**
| Tag | What work was done |
|-----|-------------------|
| `docs` | Documentation files (README, guides) |
| `changelog` | Changelog/history updates |
| `comments` | Code comments |
| `api-docs` | API documentation |
| `examples` | Code examples/samples |

**Infrastructure:**
| Tag | What work was done |
|-----|-------------------|
| `ci` | CI pipelines |
| `cd` | Deployment automation |
| `docker` | Containerization |
| `monitoring` | Logging/observability |
| `hosting` | Hosting/server config |

**Build & Config:**
| Tag | What work was done |
|-----|-------------------|
| `build` | Build system |
| `bundler` | Bundler config (webpack, vite) |
| `config` | App configuration |
| `env` | Environment variables |
| `lint` | Linter rules |
| `formatter` | Code formatter config |

**Dependencies:**
| Tag | What work was done |
|-----|-------------------|
| `dependency-add` | Added new dependency |
| `dependency-update` | Upgraded dependency |
| `dependency-remove` | Removed dependency |
| `dependency-security` | Security patch for dependency |

**Database:**
| Tag | What work was done |
|-----|-------------------|
| `database` | Database changes |
| `schema` | Schema changes |
| `data-migration` | Data migration |
| `seed` | Seed/fixture data |

**API:**
| Tag | What work was done |
|-----|-------------------|
| `api` | API changes |
| `api-breaking` | Breaking API change |
| `endpoint` | New/modified endpoint |

**Git/Process:**
| Tag | What work was done |
|-----|-------------------|
| `merge` | Merge commit |
| `revert` | Reverted previous change |
| `release` | Version bump/release |
| `init` | Initial project setup |

**Code Style:**
| Tag | What work was done |
|-----|-------------------|
| `style` | Formatting changes |
| `imports` | Organized imports |
| `whitespace` | Whitespace only changes |

**Error Handling:**
| Tag | What work was done |
|-----|-------------------|
| `error-handling` | Improved error handling |
| `logging` | Added/improved logging |
| `validation` | Input validation |

### Complexity Score (1-5)

Based on scope and impact of changes:

| Score | Description |
|-------|-------------|
| 1 | Trivial - single file, minor change |
| 2 | Small - few files, straightforward change |
| 3 | Medium - multiple files, moderate complexity |
| 4 | Large - many files, significant changes |
| 5 | Major - extensive changes, high complexity |

### Urgency Score (1-5)

Based on how critical the change was â€” reactive vs proactive work:

| Score | Level | Description | Signals |
|-------|-------|-------------|---------|
| 1 | **Planned** | Scheduled work, no time pressure | Feature roadmap, refactoring, tech debt cleanup |
| 2 | **Normal** | Regular development pace | Standard bug fixes, improvements, maintenance |
| 3 | **Elevated** | Needs attention soon | Bug affecting some users, approaching deadline |
| 4 | **Urgent** | High priority, blocking work | Breaking functionality, blocking other devs |
| 5 | **Critical** | Drop everything | Production down, security vulnerability, hotfix |

**Indicators to look for:**
- Keywords: "urgent", "hotfix", "critical", "ASAP", "emergency", "breaking"
- Tags: `hotfix`, `vulnerability`, `security` suggest higher urgency
- Time context: Weekend/after-hours commits may indicate urgency
- Severity words: "crash", "down", "broken", "blocking"

**Default:** Most commits are urgency 2 (normal development pace)

### Impact Category

Based on who/what is affected by the change:

| Impact | Description | Examples |
|--------|-------------|----------|
| `internal` | Only affects developers, no user impact | Tests, refactoring, docs, tooling, CI/CD, code comments |
| `user-facing` | Directly affects end users | UI changes, features, bug fixes users experience, UX |
| `infrastructure` | Affects deployment, hosting, operations | CI/CD, Docker, monitoring, hosting config, env vars |
| `api` | Affects external integrations or contracts | API endpoints, breaking changes, webhooks |

**Guidelines:**
- A commit can only have ONE impact category (choose the primary)
- If a commit touches both user-facing and internal (e.g., feature + tests), choose `user-facing`
- `merge` commits are typically `internal` unless the PR description indicates otherwise
- Documentation about user features is `user-facing`; dev docs are `internal`

### Examples

| Commit Subject | Tags | Complexity | Urgency | Impact |
|----------------|------|------------|---------|--------|
| "Add dark mode toggle" | `feature`, `ui` | 2 | 1 | user-facing |
| "Fix crash on startup" | `bugfix` | 2 | 4 | user-facing |
| "Fix XSS vulnerability in login" | `bugfix`, `vulnerability`, `sanitization` | 3 | 5 | user-facing |
| "Refactor auth module" | `refactor`, `types` | 3 | 1 | internal |
| "Update session notes" | `docs` | 1 | 2 | internal |
| "Add comments explaining auth flow" | `comments` | 1 | 1 | internal |
| "Merge pull request #42" | `merge` | 1 | 2 | internal |
| "Upgrade React to v19" | `dependency-update`, `migration` | 4 | 2 | internal |
| "Add user API endpoint" | `feature`, `endpoint`, `api` | 3 | 2 | api |
| "Optimize image loading" | `performance`, `ux` | 2 | 2 | user-facing |
| "HOTFIX: Production DB timeout" | `hotfix`, `database`, `performance` | 2 | 5 | infrastructure |
| "Add GitHub Actions workflow" | `ci` | 2 | 1 | infrastructure |

---

## Data Schema

### Commit Object

```json
{
  "hash": "abc123",
  "author_id": "john-doe",
  "timestamp": "2026-01-19T10:30:00Z",
  "message": "Add user authentication flow",
  "tags": ["feature", "security"],
  "complexity": 3,
  "urgency": 2,
  "impact": "user-facing",
  "files_changed": 5,
  "lines_added": 120,
  "lines_deleted": 15
}
```

**Field descriptions:**
- `tags` - Array of strings from the tag list (multiple allowed)
- `complexity` - Integer 1-5 (scope/difficulty of change)
- `urgency` - Integer 1-5 (how critical was this change)
- `impact` - One of: `internal`, `user-facing`, `infrastructure`, `api`

### Metadata Object

```json
{
  "repo_id": "my-repo",
  "extracted_at": "2026-01-19T12:00:00Z",
  "commit_count": 150,
  "authors": {
    "john-doe": {
      "name": "John Doe",
      "email": "john@example.com"
    }
  }
}
```

---

*Last updated: 2026-01-23 - Updated batch size to 25, fixed markdown formatting*
